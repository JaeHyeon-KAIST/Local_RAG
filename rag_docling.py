import streamlit as st  # ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import PyPDFLoader  # PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë„êµ¬
from langchain_docling import DoclingLoader  # ë…¼ë¬¸ ë¡œë“œë¥¼ ìœ„í•œ Docling ë¡œë”
from langchain_docling.loader import ExportType  # Docling ë‚´ë³´ë‚´ê¸° íƒ€ì…
from langchain_text_splitters import MarkdownHeaderTextSplitter  # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• 
from langchain.text_splitter import RecursiveCharacterTextSplitter  # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” ë„êµ¬
from langchain_community.embeddings import OllamaEmbeddings  # Ollamaë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
from langchain_community.vectorstores import Chroma  # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì†Œ
from langchain_community.llms import Ollama  # Ollama LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸) ì¸í„°í˜ì´ìŠ¤
from langchain.chains import RetrievalQA  # langchainì˜ RetrievalQA ì²´ì¸
from langchain.prompts import PromptTemplate  # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
from langchain.chains import LLMChain  # LLM ì²´ì¸
import tempfile  # ì„ì‹œ íŒŒì¼ ì²˜ë¦¬
import os  # íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…
import time  # ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€
import subprocess  # í„°ë¯¸ë„ ëª…ë ¹ ì‹¤í–‰
import shutil  # ë””ë ‰í† ë¦¬ ì‚­ì œ ë“± íŒŒì¼ ì‘ì—…

# Streamlit í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(
  page_title="AI ë…¼ë¬¸ ë¶„ì„ê¸°",
  page_icon="ğŸ“š",
  layout="wide"
)

# ë©”ì¸ í˜ì´ì§€ ì œëª©
st.title("ğŸ“š AI ë…¼ë¬¸ ë¶„ì„ ë„ìš°ë¯¸ with Ollama")


# 1ï¸âƒ£ ë¡œì»¬ì— ì„¤ì¹˜ëœ Ollama ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_ollama_models():
  try:
    result = subprocess.run("ollama list | sort -k1", capture_output=True, text=True, shell=True)
    models = [line.split()[0] for line in result.stdout.splitlines() if line]
    return models[1:]
  except Exception as e:
    st.error(f"Ollama ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    return []


# ì‚¬ìš©ìê°€ ë³´ìœ í•œ Ollama ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
available_models = get_ollama_models()

# ëª¨ë¸ ì„ íƒ UI (ê¸°ë³¸ê°’: deepseek-r1:14b)
selected_model = st.sidebar.selectbox(
  "ì¶”ë¡ (Reasoning)ì— ì‚¬ìš©í•  Ollama ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
  available_models,
  index=available_models.index("deepseek-r1:14b") if "deepseek-r1:14b" in available_models else 0,
  key="reasoning_model"
)

# ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ UI
answer_model = st.sidebar.selectbox(
  "ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  Ollama ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
  available_models,
  index=available_models.index("exaone3.5:latest") if "exaone3.5:latest" in available_models else 0,
  key="answer_model"
)

# ì„ë² ë”© ëª¨ë¸ ì„ íƒ UI ì¶”ê°€
embedding_model = st.sidebar.selectbox(
  "í…ìŠ¤íŠ¸ ì„ë² ë”©ì— ì‚¬ìš©í•  ì¸ì½”ë” ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
  ["nomic-embed-text", "bge-m3"],
  index=0,
  key="embedding_model"
)

# PDF ë¡œë” ì„ íƒ UI
loader_type = st.sidebar.radio(
  "PDF ë¡œë” ì„ íƒ:",
  ["PyPDFLoader", "DoclingLoader"],
  help="PyPDFLoaderëŠ” ì¼ë°˜ì ì¸ PDF ì²˜ë¦¬ì— ì í•©í•˜ê³ , DoclingLoaderëŠ” í•™ìˆ  ë…¼ë¬¸ì˜ êµ¬ì¡°ë¥¼ ë” ì˜ ì¸ì‹í•©ë‹ˆë‹¤."
)

# PDF íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
uploaded_file = st.file_uploader("ë…¼ë¬¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf'])

# 2ï¸âƒ£ íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì‹¤í–‰ë˜ëŠ” ë¡œì§
if uploaded_file is not None:
  # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
  with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
    tmp_file.write(uploaded_file.getvalue())
    temp_path = tmp_file.name  # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì €ì¥

  try:
    # ì„ íƒí•œ ë¡œë”ì— ë”°ë¼ PDF íŒŒì¼ ì²˜ë¦¬
    if loader_type == "PyPDFLoader":
      with st.spinner('PyPDFLoaderë¡œ ë…¼ë¬¸ì„ ë¡œë“œ ì¤‘...'):
        # PDF íŒŒì¼ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
        loader = PyPDFLoader(temp_path)
        pages = loader.load()

        # í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(pages)

    else:  # DoclingLoader
      with st.spinner('DoclingLoaderë¡œ ë…¼ë¬¸ì„ ë¡œë“œ ì¤‘...'):
        # Docling ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ì—ì„œ ë…¼ë¬¸ ë¡œë“œ
        loader = DoclingLoader(
          file_path=temp_path,
          export_type=ExportType.MARKDOWN
        )
        docs = loader.load()

        # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• 
        header_splitter = MarkdownHeaderTextSplitter(
          headers_to_split_on=[
            ("#", "Header_1"),
            ("##", "Header_2"),
            ("###", "Header_3"),
          ]
        )

        # ê° ë¬¸ì„œë¥¼ í—¤ë” ê¸°ë°˜ìœ¼ë¡œ ë¶„í• 
        splits = []
        for doc in docs:
          try:
            header_splits = header_splitter.split_text(doc.page_content)
            splits.extend(header_splits)
          except Exception as e:
            st.warning(f"í—¤ë” ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê¸°ë³¸ ë¶„í•  ë°©ì‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            # í—¤ë” ë¶„í• ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ë¶„í•  ë°©ì‹ ì‚¬ìš©
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_text(doc.page_content)
            from langchain_core.documents import Document

            splits.extend([Document(page_content=chunk) for chunk in chunks])

    # ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´ ì„ë² ë”© ë° ì§ˆë¬¸ ì²˜ë¦¬ ì§„í–‰
    if 'splits' in locals():
      # 3ï¸âƒ£ ì„ íƒí•œ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì„¤ì •
      embeddings = OllamaEmbeddings(
        model=embedding_model,  # ì‚¬ìš©ìê°€ ì„ íƒí•œ ì„ë² ë”© ëª¨ë¸
        base_url="http://localhost:11434"
      )

      # ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ë° ë¬¸ì„œ ì €ì¥
      persist_directory = "./.chroma"

      # ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
      vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=persist_directory)

      # 4ï¸âƒ£ ì„ íƒí•œ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ LLM ì„¤ì •
      reasoning_llm = Ollama(
        model=selected_model,  # ì‚¬ìš©ìê°€ ì„ íƒí•œ ì¶”ë¡  ëª¨ë¸ ë°˜ì˜
        stop=["</think>"],
        base_url="http://localhost:11434"
      )

      answer_llm = Ollama(
        model=answer_model,  # ì‚¬ìš©ìê°€ ì„ íƒí•œ ë‹µë³€ ìƒì„± ëª¨ë¸ ë°˜ì˜
        temperature=0,
        base_url="http://localhost:11434"
      )

      # RetrievalQA ì²´ì¸ ì„¤ì • (ì¶”ë¡ ìš©)
      retrieval_chain = RetrievalQA.from_chain_type(
        llm=reasoning_llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
      )

      # ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
      answer_template = """
          ë‹¹ì‹ ì€ í•™ìˆ  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

          ë‹¤ìŒì€ ì§ˆë¬¸ì— ëŒ€í•œ ì´ˆê¸° ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:
          {reasoning_result}

          ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:
          {question}

          ë¬¸ì„œì˜ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ë˜, í•™ìˆ ì ì´ê³  ì •í™•í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
          """

      answer_prompt = PromptTemplate(
        template=answer_template,
        input_variables=["reasoning_result", "question"]
      )

      answer_chain = LLMChain(
        llm=answer_llm,
        prompt=answer_prompt
      )

      # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°
      user_question = st.text_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”:")

      # ì§ˆë¬¸ì´ ì…ë ¥ë˜ë©´ ì‹¤í–‰
      if user_question:
        start_time = time.time()

        with st.spinner('ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'):
          progress_placeholder = st.empty()
          progress_placeholder.text("1/2 ë‹¨ê³„: ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ë° ì¶”ë¡  ì¤‘...")

          # 1ë‹¨ê³„: ì¶”ë¡  ëª¨ë¸ë¡œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ë° ë¶„ì„
          reasoning_result = retrieval_chain({"query": user_question})
          reasoning_output = reasoning_result["result"]

          progress_placeholder.text("2/2 ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")

          # 2ë‹¨ê³„: ë‹µë³€ ìƒì„± ëª¨ë¸ë¡œ ìµœì¢… ë‹µë³€ ë§Œë“¤ê¸°
          final_answer = answer_chain.run(
            reasoning_result=reasoning_output,
            question=user_question
          )

          end_time = time.time()
          elapsed_time = round(end_time - start_time, 2)

          progress_placeholder.empty()

          # ë‹µë³€ ì¶œë ¥
          st.write("### ğŸ¤– AI Answer:")
          st.write(final_answer)

          # ìƒì„¸ ì •ë³´ í™•ì¸ ì˜µì…˜ (ì ‘ì´ì‹)
          with st.expander("ì¶”ë¡  ê³¼ì • ìƒì„¸ ë³´ê¸°"):
            st.markdown("#### ì¶”ë¡  ëª¨ë¸ì˜ ë¶„ì„ ê²°ê³¼:")
            st.write(reasoning_output)

          st.info(f"â±ï¸ ë‹µë³€ ìƒì„± ì†Œìš” ì‹œê°„: {elapsed_time}ì´ˆ")

  finally:
    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    os.unlink(temp_path)

# ì‚¬ì´ë“œë°”ì— ì‚¬ìš© ì„¤ëª… ì¶”ê°€
with st.sidebar:
  st.header("ì‚¬ìš© ë°©ë²•")
  st.write("""
    1. ì¶”ë¡ (Reasoning)ì— ì‚¬ìš©í•  ëª¨ë¸ê³¼ ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.
    2. ì„ë² ë”©ì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ì¸ì½”ë” ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:
       - nomic-embed-text: ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì„ë² ë”©ì— ì í•©
       - bge-m3: ë‹¤êµ­ì–´ ì§€ì› ë° ë” ì •í™•í•œ ì˜ë¯¸ ê²€ìƒ‰ì— ê°•ì 
    3. PDF ë¡œë” íƒ€ì…ì„ ì„ íƒí•˜ì„¸ìš”:
       - PyPDFLoader: ì¼ë°˜ì ì¸ PDF íŒŒì¼ ì²˜ë¦¬
       - DoclingLoader: í•™ìˆ  ë…¼ë¬¸ êµ¬ì¡° ì¸ì‹ì— ìµœì í™”
    4. PDF ë…¼ë¬¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.
    5. ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš”.
    6. AIê°€ ë‘ ë‹¨ê³„ë¡œ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)

  st.markdown("---")

  st.header("ë‘ ë‹¨ê³„ ì²˜ë¦¬ ê³¼ì •")
  st.write("""
    **ì¶”ë¡  ë‹¨ê³„ (Reasoning)**
    - ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    - ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ˆê¸° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    **ë‹µë³€ ìƒì„± ë‹¨ê³„ (Answer Generation)**
    - ì¶”ë¡  ë‹¨ê³„ì˜ ê²°ê³¼ë¬¼ì„ ë°”íƒ•ìœ¼ë¡œ ë” ì •ì œëœ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ë³´ë‹¤ ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)

  st.markdown("---")

  st.header("ë¬¸ì„œ ë¡œë” ë¹„êµ")
  st.write("""
    **PyPDFLoader**
    - ì¼ë°˜ì ì¸ PDF íŒŒì¼ ì²˜ë¦¬ì— ì í•©
    - ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê¸°ëŠ¥
    - ì²˜ë¦¬ ì†ë„ê°€ ë¹ ë¦„

    **DoclingLoader**
    - í•™ìˆ  ë…¼ë¬¸ êµ¬ì¡° ì¸ì‹ì— ìµœì í™”
    - ë…¼ë¬¸ì˜ í—¤ë”ì™€ ì„¹ì…˜ì„ ì¸ì‹í•˜ì—¬ ì²˜ë¦¬
    - ë” ì •í™•í•œ ë…¼ë¬¸ ë¶„ì„ì´ ê°€ëŠ¥
    - ë³µì¡í•œ ë…¼ë¬¸ êµ¬ì¡° ì²˜ë¦¬ì— ìœ ë¦¬
    """)