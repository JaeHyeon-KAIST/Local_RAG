import streamlit as st  # ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import PyPDFLoader  # PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë„êµ¬
from langchain_docling import DoclingLoader  # ë…¼ë¬¸ ë¡œë“œë¥¼ ìœ„í•œ Docling ë¡œë”
from langchain_docling.loader import ExportType  # Docling ë‚´ë³´ë‚´ê¸° íƒ€ì…
from langchain_text_splitters import MarkdownHeaderTextSplitter  # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• 
from langchain.text_splitter import RecursiveCharacterTextSplitter  # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” ë„êµ¬
from langchain.schema.runnable import Runnable
from langchain_community.vectorstores import Chroma  # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì†Œ
import tempfile  # ì„ì‹œ íŒŒì¼ ì²˜ë¦¬
import os  # íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…
import time  # ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€
import requests  # LM Studio API í˜¸ì¶œì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from chromadb.config import Settings

# Streamlit í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(
  page_title="AI ë…¼ë¬¸ ë¶„ì„ê¸°",
  page_icon="ğŸ“š",
  layout="wide"
)

# ë©”ì¸ í˜ì´ì§€ ì œëª©
st.title("ğŸ“š AI ë…¼ë¬¸ ë¶„ì„ ë„ìš°ë¯¸ with LM Studio")

# LM Studio ì„œë²„ URL
LM_STUDIO_URL = "http://127.0.0.1:1234"


# LM Studioì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_lm_studio_models():
  try:
    response = requests.get(f"{LM_STUDIO_URL}/v1/models")
    if response.status_code == 200:
      models_data = response.json()
      # API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ëª¨ë¸ ID ë˜ëŠ” ì´ë¦„ ì¶”ì¶œ
      if "data" in models_data:
        # OpenAI í˜¸í™˜ API í˜•ì‹
        return [model["id"] for model in models_data["data"]]
      else:
        # ë‹¨ìˆœ ëª©ë¡ í˜•ì‹
        return models_data
    else:
      st.warning(f"LM Studio ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ìƒíƒœ ì½”ë“œ: {response.status_code})")
      return []
  except Exception as e:
    st.warning(f"LM Studio ì—°ê²° ì˜¤ë¥˜: {e}")
    return []


# LM Studio APIë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤
class LMStudioEmbeddings:
  def __init__(self, api_url=f"{LM_STUDIO_URL}/v1/embeddings", model_name="text-embedding-nomic-embed-text-v1.5-embedding"):
    self.api_url = api_url
    self.model_name = model_name
    # ëª¨ë¸ë³„ ì„ë² ë”© ì°¨ì› ì„¤ì •
    self.embedding_dim = 1024 if "bge" in model_name.lower() else 768

  def embed_documents(self, texts):
    embeddings = []
    for i, text in enumerate(texts):
      if i == 0:  # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì„ë² ë”© ì°¨ì›ë§Œ ì¶œë ¥
        embedding = self.embed_query(text)
        embeddings.append(embedding)
      else:
        embeddings.append(self.embed_query(text))
    return embeddings

  def embed_query(self, text):
    headers = {
      "Content-Type": "application/json"
    }

    data = {
      "input": text,
      "model": self.model_name
    }

    response = requests.post(self.api_url, headers=headers, json=data)

    if response.status_code == 200:
      result = response.json()
      if "data" in result and len(result["data"]) > 0 and "embedding" in result["data"][0]:
        embedding = result["data"][0]["embedding"]
        actual_dim = len(embedding)
        if actual_dim != self.embedding_dim:
          st.warning(f"ì˜ˆìƒ ì„ë² ë”© ì°¨ì›({self.embedding_dim})ê³¼ ì‹¤ì œ ì„ë² ë”© ì°¨ì›({actual_dim})ì´ ë‹¤ë¦…ë‹ˆë‹¤.")
          self.embedding_dim = actual_dim  # ì‹¤ì œ ì°¨ì›ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        return embedding
      else:
        # ì„ë² ë”©ì´ ì œê³µë˜ì§€ ì•ŠëŠ” ê²½ìš°, ê°€ì§œ ì„ë² ë”© ë°˜í™˜
        st.warning("LM Studioì—ì„œ ì„ë² ë”©ì„ ì œê³µí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëŒ€ì²´ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return [0.0] * self.embedding_dim
    else:
      st.error(f"ì„ë² ë”© API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}, {response.text}")
      return [0.0] * self.embedding_dim


# LM Studio APIë¥¼ í†µí•´ ì¶”ë¡ í•˜ëŠ” í´ë˜ìŠ¤ ì •ì˜
class LMStudioLLM(Runnable):
  def __init__(self, api_url=f"{LM_STUDIO_URL}/v1/completions", model_name="deepseek-r1-distill-qwen-32b",
               temperature=0.1, max_tokens=2000, stop=None):
    self.api_url = api_url
    self.model_name = model_name
    self.temperature = temperature
    self.max_tokens = max_tokens
    self.stop = stop if stop else []

  def invoke(self, *args, **kwargs):
    prompt = args[0] if args else kwargs.get('prompt')
    return self.run(prompt)

  def run(self, prompt):
    headers = {
      "Content-Type": "application/json"
    }

    data = {
      "prompt": prompt,
      "model": self.model_name,
      "temperature": self.temperature,
      "max_tokens": self.max_tokens,
      "stop": self.stop
    }

    response = requests.post(self.api_url, headers=headers, json=data)

    if response.status_code == 200:
      return response.json()["choices"][0]["text"]
    else:
      raise Exception(f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}, {response.text}")

  def __call__(self, prompt):  # __call__ ë©”ì„œë“œ ì¶”ê°€
    return self.run(prompt)


# LM Studio ì„œë²„ ì—°ê²° í™•ì¸
lm_studio_available = True
lm_studio_models = []

try:
  response = requests.get(f"{LM_STUDIO_URL}/v1/models")
  if response.status_code == 200:
    lm_studio_models = get_lm_studio_models()
  else:
    lm_studio_available = False
    st.warning("LM Studio ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
except Exception as e:
  lm_studio_available = False
  st.warning(f"LM Studio ì„œë²„ ì—°ê²° ì˜¤ë¥˜: {e}")

# ì‚¬ì´ë“œë°” êµ¬ì„±
st.sidebar.header("ëª¨ë¸ ì„¤ì •")

# ì¶”ë¡  ëª¨ë¸ ì„ íƒ - LM Studio ì‚¬ìš©
st.sidebar.subheader("ì¶”ë¡ (Reasoning) ëª¨ë¸ ì„¤ì •")
if lm_studio_available and lm_studio_models:
  reasoning_model = st.sidebar.selectbox(
    "LM Studio ì¶”ë¡  ëª¨ë¸ ì„ íƒ:",
    lm_studio_models,
    index=lm_studio_models.index(
      "deepseek-r1-distill-qwen-32b") if "deepseek-r1-distill-qwen-32b" in lm_studio_models else 0,
    key="reasoning_model"
  )
else:
  st.sidebar.warning("LM Studio ì—°ê²° ë¶ˆê°€")
  reasoning_model = ""

# ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ UI
st.sidebar.subheader("ë‹µë³€ ìƒì„± ëª¨ë¸ ì„¤ì •")
if lm_studio_available and lm_studio_models:
  answer_model = st.sidebar.selectbox(
    "LM Studio ë‹µë³€ ìƒì„± ëª¨ë¸ ì„ íƒ:",
    lm_studio_models,
    index=lm_studio_models.index("exaone-3.5-7.8b-instruct") if "exaone-3.5-7.8b-instruct" in lm_studio_models else 0,
    key="answer_model"
  )
else:
  st.sidebar.warning("LM Studio ì—°ê²° ë¶ˆê°€")
  answer_model = ""

# ì„ë² ë”© ëª¨ë¸ ì„ íƒ UI ì¶”ê°€
embedding_model = st.sidebar.selectbox(
  "í…ìŠ¤íŠ¸ ì„ë² ë”©ì— ì‚¬ìš©í•  ì¸ì½”ë” ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
  ["text-embedding-nomic-embed-text-v1.5-embedding", "text-embedding-bge-m3"],
  index=0,
  key="embedding_model"
)

# PDF ë¡œë” ì„ íƒ UI
loader_type = st.sidebar.radio(
  "PDF ë¡œë” ì„ íƒ:",
  ["PyPDFLoader", "DoclingLoader"],
  help="PyPDFLoaderëŠ” ì¼ë°˜ì ì¸ PDF ì²˜ë¦¬ì— ì í•©í•˜ê³ , DoclingLoaderëŠ” í•™ìˆ  ë…¼ë¬¸ì˜ êµ¬ì¡°ë¥¼ ë” ì˜ ì¸ì‹í•©ë‹ˆë‹¤."
)

# PDF íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
uploaded_file = st.file_uploader("ë…¼ë¬¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf'])

vectorstore = None

# 2ï¸âƒ£ íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì‹¤í–‰ë˜ëŠ” ë¡œì§
if uploaded_file is not None:
  # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
  with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
    tmp_file.write(uploaded_file.getvalue())
    temp_path = tmp_file.name  # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì €ì¥

  try:
    # LM Studio ì„œë²„ ì—°ê²° í™•ì¸
    if not lm_studio_available:
      st.error("LM Studio ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
      st.stop()

    if loader_type == "PyPDFLoader":
      with st.spinner('PyPDFLoaderë¡œ ë…¼ë¬¸ì„ ë¡œë“œ ì¤‘...'):
        # PDF íŒŒì¼ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
        loader = PyPDFLoader(temp_path)
        pages = loader.load()

        # í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(pages)

    else:  # DoclingLoader
      with st.spinner('DoclingLoaderë¡œ ë…¼ë¬¸ì„ ë¡œë“œ ì¤‘...'):
        # Docling ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ì—ì„œ ë…¼ë¬¸ ë¡œë“œ
        loader = DoclingLoader(
          file_path=temp_path,
          export_type=ExportType.MARKDOWN
        )
        docs = loader.load()

        # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• 
        header_splitter = MarkdownHeaderTextSplitter(
          headers_to_split_on=[
            ("#", "Header_1"),
            ("##", "Header_2"),
            ("###", "Header_3"),
          ]
        )

        # ê° ë¬¸ì„œë¥¼ í—¤ë” ê¸°ë°˜ìœ¼ë¡œ ë¶„í• 
        splits = []
        for doc in docs:
          try:
            header_splits = header_splitter.split_text(doc.page_content)
            splits.extend(header_splits)
          except Exception as e:
            st.warning(f"í—¤ë” ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê¸°ë³¸ ë¶„í•  ë°©ì‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            # í—¤ë” ë¶„í• ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ë¶„í•  ë°©ì‹ ì‚¬ìš©
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_text(doc.page_content)
            from langchain_core.documents import Document

            splits.extend([Document(page_content=chunk) for chunk in chunks])

    # Try to set up embeddings and vector store
    try:
      # ì„ë² ë”© ì„¤ì •
      embeddings = LMStudioEmbeddings(
        api_url=f"{LM_STUDIO_URL}/v1/embeddings",
        model_name=embedding_model
      )

      # ì²« ë²ˆì§¸ ë¬¸ì„œë¡œ ì‹¤ì œ ì„ë² ë”© ì°¨ì› í™•ì¸
      if splits:
        first_embedding = embeddings.embed_query(splits[0].page_content)
        actual_dim = len(first_embedding)
      else:
        actual_dim = embeddings.embedding_dim

      # ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ ë²¡í„° ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ ì„¤ì •
      persist_directory = f"./chroma{embedding_model}"

      # Chroma ì„¤ì • ì—…ë°ì´íŠ¸ (íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ë°˜)
      chroma_settings = Settings(
          anonymized_telemetry=False,
          is_persistent=True,  # íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥
          persist_directory=persist_directory  # ëª¨ë¸ë³„ ì €ì¥ ê²½ë¡œ ì§€ì •
      )

      # ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
      vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        collection_metadata={"dimension": actual_dim},
        client_settings=chroma_settings
      )

      if vectorstore is None:
        raise Exception("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨")
      
    except Exception as e:
      st.error(f"ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ì˜¤ë¥˜: {e}")
      st.warning("ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ ì—†ì´ ì „ì²´ ë¬¸ì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
      vectorstore = None

    # 4ï¸âƒ£ LM Studioë¥¼ í†µí•œ ì¶”ë¡  LLM ì„¤ì •
    reasoning_llm = LMStudioLLM(
      api_url=f"{LM_STUDIO_URL}/v1/completions",
      model_name=reasoning_model,
      temperature=0,
      max_tokens=2000,
      stop=["</think>"]
    )

    # ë‹µë³€ ìƒì„±ìš© Ollama LLM ì„¤ì •
    answer_llm = LMStudioLLM(
      api_url=f"{LM_STUDIO_URL}/v1/completions",
      model_name=answer_model,
      temperature=0,
      max_tokens=1000
    )


    # RetrievalQA ì²´ì¸ í´ë˜ìŠ¤ ì •ì˜
    class CustomRetrievalQA:
      def __init__(self, llm, retriever=None, documents=None):
        self.llm = llm
        self.retriever = retriever
        self.documents = documents

      def __call__(self, query_dict):
        query = query_dict["query"]

        if self.retriever:
          # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
          docs = self.retriever.get_relevant_documents(query)
          context = "\n\n".join([doc.page_content for doc in docs])
        else:
          # ë²¡í„° ê²€ìƒ‰ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ì „ì²´ ë¬¸ì„œ ì‚¬ìš©
          if self.documents:
            # ëª¨ë“  ë¬¸ì„œì˜ ë‚´ìš©ì„ ê²°í•©í•˜ë˜ ìµœëŒ€ ê¸¸ì´ ì œí•œ
            all_text = "\n\n".join([doc.page_content for doc in self.documents])
            # í† í° ê¸¸ì´ ì œí•œì„ ìœ„í•´ í…ìŠ¤íŠ¸ truncate (ëŒ€ëµ 10,000ì ì •ë„ë¡œ)
            context = all_text[:10000] + ("..." if len(all_text) > 10000 else "")
            docs = self.documents[:5]  # ì²˜ìŒ 5ê°œ ë¬¸ì„œë§Œ ì°¸ì¡°ìš©ìœ¼ë¡œ ì €ì¥
          else:
            context = "ë¬¸ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            docs = []

        prompt_text = f"""
            ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ì„ ë¶„ì„í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë…¼ë¬¸ ë‚´ìš©ì…ë‹ˆë‹¤:

            {context}

            <think>
            ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
            {query}

            ë¬¸ì„œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ê³„ë³„ë¡œ ì¶”ë¡ í•´ì„œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
            </think>
            """

        result = self.llm(prompt_text)

        return {
          "query": query,
          "result": result,
          "source_documents": docs
        }


    if vectorstore:
      retrieval_chain = CustomRetrievalQA(
        llm=reasoning_llm,
        retriever=vectorstore.as_retriever()
      )
    else:
      st.error("Vector store is not available. Please check the embeddings and vector store creation process.")
      st.stop()

    # ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    answer_template = """
        ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ì„ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

        ë‹¤ìŒì€ ì§ˆë¬¸ì— ëŒ€í•œ ì´ˆê¸° ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:
        {reasoning_result}

        ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:
        {question}

        ë…¼ë¬¸ì˜ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ë˜, í•™ìˆ ì ì´ê³  ì •í™•í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
        """


    def generate_final_answer(llm, reasoning_result, question):
      prompt_text = answer_template.format(
        reasoning_result=reasoning_result,
        question=question
      )
      return llm(prompt_text)


    # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°
    user_question = st.text_input("ë…¼ë¬¸ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”:")

    # ì§ˆë¬¸ì´ ì…ë ¥ë˜ë©´ ì‹¤í–‰
    if user_question:
      start_time = time.time()

      with st.spinner('ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'):
        progress_placeholder = st.empty()
        progress_placeholder.text("1/2 ë‹¨ê³„: ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ë° ì¶”ë¡  ì¤‘...")

        # 1ë‹¨ê³„: ì¶”ë¡  ëª¨ë¸ë¡œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ë° ë¶„ì„
        reasoning_result = retrieval_chain({"query": user_question})
        reasoning_output = reasoning_result["result"]

        progress_placeholder.text("2/2 ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")

        # 2ë‹¨ê³„: ë‹µë³€ ìƒì„± ëª¨ë¸ë¡œ ìµœì¢… ë‹µë³€ ë§Œë“¤ê¸°
        final_answer = generate_final_answer(
          answer_llm,
          reasoning_output,
          user_question
        )

        end_time = time.time()
        elapsed_time = round(end_time - start_time, 2)

        progress_placeholder.empty()

        # ë‹µë³€ ì¶œë ¥
        st.write("### ğŸ¤– AI Answer:")
        st.write(final_answer)

        # ìƒì„¸ ì •ë³´ í™•ì¸ ì˜µì…˜ (ì ‘ì´ì‹)
        with st.expander("ì¶”ë¡  ê³¼ì • ìƒì„¸ ë³´ê¸°"):
          st.markdown("#### ì¶”ë¡  ëª¨ë¸ì˜ ë¶„ì„ ê²°ê³¼:")
          st.write(reasoning_output)

          st.markdown("#### ì°¸ì¡°ëœ ë¬¸ì„œ ì¶œì²˜:")
          for i, doc in enumerate(reasoning_result["source_documents"]):
            st.markdown(f"**ë¬¸ì„œ {i + 1}:**")
            st.text(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
            st.markdown("---")

        st.info(f"â±ï¸ ë‹µë³€ ìƒì„± ì†Œìš” ì‹œê°„: {elapsed_time}ì´ˆ")

  finally:
    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    os.unlink(temp_path)

# ì‚¬ì´ë“œë°”ì— ì‚¬ìš© ì„¤ëª… ì¶”ê°€
with st.sidebar:
  st.header("ì‚¬ìš© ë°©ë²•")
  st.write("""
    1. ì¶”ë¡ (Reasoning)ì— ì‚¬ìš©ë  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.
    2. ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.
    3. PDF ë…¼ë¬¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.
    4. ë…¼ë¬¸ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš”.
    5. AIê°€ ë‘ ë‹¨ê³„ë¡œ ë…¼ë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤:
       - ì¶”ë¡  ëª¨ë¸ì´ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
       - ë‹µë³€ ìƒì„± ëª¨ë¸ì´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """)

  st.markdown("---")

  st.header("ë‘ ë‹¨ê³„ ì²˜ë¦¬ ê³¼ì •")
  st.write("""
    **ì¶”ë¡  ë‹¨ê³„ (Reasoning)**
    - ë…¼ë¬¸ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    - ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ˆê¸° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    **ë‹µë³€ ìƒì„± ë‹¨ê³„ (Answer Generation)**
    - ì¶”ë¡  ë‹¨ê³„ì˜ ê²°ê³¼ë¬¼ì„ ë°”íƒ•ìœ¼ë¡œ ë” ì •ì œëœ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ë³´ë‹¤ ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)
